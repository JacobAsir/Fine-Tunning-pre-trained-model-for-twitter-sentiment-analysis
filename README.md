# Fine-Tunning-pre-trained-model-for-twitter-sentiment-analysis

I recently worked on a project that involved fine-tuning pre-trained models on a Twitter sentiment dataset.

ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ—œğ—»ğ˜ƒğ—¼ğ—¹ğ˜ƒğ—²ğ—±:
ğ——ğ—®ğ˜ğ—® ğ—£ğ—¿ğ—²ğ—½ğ—®ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»: Leveraged the "mteb/tweet_sentiment_extraction" dataset for training and evaluation from hugging face.

ğ—§ğ—¼ğ—¸ğ—²ğ—»ğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—»: Employed AutoTokenizer for both models to tokenize the tweets effectively, ensuring input consistency.

ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—™ğ—¶ğ—»ğ—²-ğ—§ğ˜‚ğ—»ğ—¶ğ—»ğ—´:
ğ— ğ—¶ğ—»ğ—¶ğ—Ÿğ— -ğ—ŸğŸ­ğŸ®-ğ—›ğŸ¯ğŸ´ğŸ°-ğ˜‚ğ—»ğ—°ğ—®ğ˜€ğ—²ğ—±: Fine-tuned with a focus on optimizing for a 3-label classification (Negative, Neutral, Positive).
ğ——ğ—²ğ—•ğ—˜ğ—¥ğ—§ğ—®-ğ—¯ğ—®ğ˜€ğ—²-ğ—ºğ—»ğ—¹ğ—¶: Similarly fine-tuned to enhance sentiment classification capabilities.

ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´: Configured training with a learning rate of 2e-5 over two epochs, incorporating strategies like weight decay and evaluation at each epoch to ensure robust model performance.

ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—¶ğ—¼ğ—»: Used an accuracy metric to validate model predictions against true labels, ensuring model reliability.

ğ——ğ—²ğ—•ğ—˜ğ—¥ğ—§ğ—®-ğ—¯ğ—®ğ˜€ğ—²-ğ—ºğ—»ğ—¹ğ—¶ was chosen due to its high accuracy in making predictions and returning precise results.

ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—ºğ—²ğ—»ğ˜: Deployed the fine-tuned models on Hugging Face Hub for public use. Sentiment analysis pipelines were created to demonstrate the models' capabilities in real-world sentiment detection.
